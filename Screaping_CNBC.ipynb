{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXpLAcHIE94_"
      },
      "outputs": [],
      "source": [
        "import requests as req\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from datetime import datetime\n",
        "import csv\n",
        "\n",
        "hades = { 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_cnbc(hal):\n",
        "  global hades\n",
        "  a = 1\n",
        "  for page in range(1, hal+1):\n",
        "    url_cnbc = f\"https://www.cnbcindonesia.com/search?query=bea%20cukai&p={page}&kanal=&tipe=artikel&date=\"\n",
        "    ge = req.get(url_cnbc, headers=hades).text\n",
        "    sop = bs(ge, 'lxml')\n",
        "    ul = sop.find('ul', class_='list media_rows middle thumb terbaru gtm_indeks_feed')\n",
        "    if ul is None:\n",
        "      continue\n",
        "    li = ul.find_all('li')\n",
        "    for x in li:\n",
        "      link_art = x.find('article').find('a')['href']\n",
        "      ge_ = req.get(link_art, headers=hades).text\n",
        "      sop_ = bs(ge_, 'lxml')\n",
        "      art_div = sop_.find('div', class_='lm_content mt10')\n",
        "      if art_div is None:\n",
        "        continue\n",
        "      art = art_div.find('article')\n",
        "      if art is None:\n",
        "        continue\n",
        "      headline = art.find('h1').text\n",
        "      date_text = art.find('div', class_='date').text\n",
        "      date = datetime.strptime(date_text, '%d %B %Y %H:%M')\n",
        "      warp = art.find_all('div', class_='detail_text')\n",
        "      for w in warp:\n",
        "        paragraphs = w.find_all('p')\n",
        "        content = ''.join(p.text for p in paragraphs).replace('\\n', '').replace('ADVERTISEMENT', '').replace('SCROLL TO RESUME CONTENT', '')\n",
        "        print(f'done[{a}] > {headline[0:10]}')\n",
        "        a += 1\n",
        "        with open('cnbc_result.csv', 'a', newline='', encoding='utf-8') as file:\n",
        "          wr = csv.writer(file, delimiter=',')\n",
        "          wr.writerow([headline, date, link_art, content])"
      ],
      "metadata": {
        "id": "sWaHGfyFHAUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrape_cnbc(4)"
      ],
      "metadata": {
        "id": "Jws-Xjk9Zynt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}